{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import urllib\n",
    "import csv\n",
    "import datetime\n",
    "import sys\n",
    "import requests\n",
    "import json\n",
    "import pandas\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import glob\n",
    "#from forex_python.converter import CurrencyRates\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentdir = 'c:\\\\Users\\\\303435\\\\OneDrive - The Warehouse Group\\\\Desktop\\\\Temp\\\\Data\\\\MAX\\\\py_venv\\\\PriceCheckerV2'\n",
    "search_list = currentdir+'\\SearchList.txt'\n",
    "#user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36' #Change to different agent if 403 error code return, it may the server/website identify you are webscrapping \n",
    "#user_agent = 'python-requests/2.28.1'\n",
    "#user_agent = 'Mozilla/5.0 (X11; Linux x86_64; rv:95.0) Gecko/20100101 Firefox/95.0'\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "headers={'User-Agent':user_agent} \n",
    "url_template = 'https://cdkeyprices.com/search?sk={name}&categories=&platforms=all_xbox&types=game&filter=1' \n",
    "extract_file_name = 'Price_List.csv'\n",
    "result_datetime = datetime.datetime.now(tz=None).strftime(\"%d%m%Y_%H%M\")\n",
    "\n",
    "last_week_date_raw = datetime.datetime.today() + datetime.timedelta(days=-7)\n",
    "last_week_date = last_week_date_raw.strftime(\"%d-%m-%Y\") \n",
    "last_week_files = []\n",
    "files = glob.iglob(currentdir + '\\\\file\\\\*', recursive=False) \n",
    "for i in files:\n",
    "  i2 = i.split('\\\\')[-1]\n",
    "  i3 = i2.split('_')[-2]\n",
    "  if i3 > last_week_date:\n",
    "      last_week_files.append(i2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting_file(last_week_files):\n",
    "  last_week_files= last_week_files\n",
    "  sort_order = []\n",
    "  \n",
    "  for i in last_week_files:\n",
    "      i2 = datetime.datetime.strptime(i.split('_')[2].split('.')[0][0:2]+'-'+i.split('_')[2].split('.')[0][2:4]+'-'+i.split('_')[2].split('.')[0][4:], '%d-%m-%Y')\n",
    "      sort_order.append(i2) \n",
    "  zipped = list(zip(last_week_files,sort_order))\n",
    "  sorted_files = sorted(zipped, key = lambda x: x[1])\n",
    "  sorted_files_trim = []\n",
    "  for i in sorted_files:\n",
    "    sorted_files_trim.append(i[0])\n",
    "  return sorted_files_trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_v2(search_list):\n",
    "  name_list= []\n",
    "  url_list = []\n",
    "  f = open(search_list,\"r\")\n",
    "  for i in f:\n",
    "      if 'bought' in i:\n",
    "          print('skip as '+i+ ' already')\n",
    "          continue\n",
    "      name = i.replace(\"'\", \"\")\n",
    "      n_name = name.replace(\"\\n\",\"\")\n",
    "      r_name = n_name.replace(\" \",\"+\")\n",
    "      name_list.append(r_name)\n",
    "  f.close()  \n",
    "   \n",
    "  for i in name_list:\n",
    "    url = url_template.format(name = i)\n",
    "    if url == 'https://cdkeyprices.com/item/dragon-quest-xi-echoes-of-an-elusive-age-cd-key-xbox':\n",
    "      continue\n",
    "    url_list.append(url)\n",
    "  print('url list and name list are generated')       \n",
    "  return url_list, name_list\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(url_list):\n",
    "  result_list = []\n",
    "  price_usd_list = []\n",
    "  captured_timestamp_list = []\n",
    "  release_yr = []\n",
    "  url = []\n",
    "  for search_url in url_list:\n",
    "    #request=urllib.request.Request(search_url,None,headers)\n",
    "    #response = urllib.request.urlopen(request)\n",
    "    #data = response.read()\n",
    "    response = requests.get(search_url)\n",
    "    if response.status_code ==200:\n",
    "      data = response.text\n",
    "      soup = BeautifulSoup(data,'html.parser')\n",
    "      link = soup.findAll(\"div\", {\"class\": \"product-list-price buy_cell\"})\n",
    "      link_yr = soup.findAll(\"div\", {\"class\": \"product-list-year\"})\n",
    "    else:\n",
    "      print(f\"Failed to get data via {url_list}\")\n",
    "    \n",
    "    for i in link_yr:\n",
    "      release_yr.append(i.string)\n",
    "    for i in link:\n",
    "      result_name = i.contents[1]['href'].replace(\"https://cdkeyprices.com/item/\",\"\").replace   (\"-cd-key-xbox\",\"\").replace(\"-\",\" \")\n",
    "      t = i.contents[1]\n",
    "      #print(t)\n",
    "      if len(t) !=2:#in case there is a result return without price data\n",
    "        print('skip '+(result_name.title())+ ' as no data returned')\n",
    "        continue\n",
    "      url.append(search_url)\n",
    "      result_list.append((result_name.title()))\n",
    "      t2 = t.contents[1]\n",
    "      #price_usd_list.append(i.contents[1].contents[1]['data-price-usd-value'])\n",
    "      price_usd_list.append(t2['data-price-usd-value'])\n",
    "      captured_timestamp = datetime.datetime.now(tz=None)\n",
    "      captured_timestamp_list.append(captured_timestamp.strftime(\"%x %X\"))\n",
    "      \n",
    "  combined_list = list(zip( result_list,release_yr,price_usd_list,captured_timestamp_list ))\n",
    "  return combined_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def get_currency(original_currency, converted_currency):\n",
    "#  c = CurrencyRates()\n",
    "#  exchange_rate = c.get_rate(original_currency, converted_currency)\n",
    "#  return round(exchange_rate,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\303435\\OneDrive - The Warehouse Group\\Desktop\\Temp\\Data\\MAX\\py_venv\\PriceCheckerV2\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.freeforexapi.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USD_NZD: 1.62\n"
     ]
    }
   ],
   "source": [
    "#There is a bug on forex api, so have to use it manually\n",
    "\n",
    "fx_url = \"https://www.freeforexapi.com/api/live?pairs=USDNZD\"\n",
    "\n",
    "payload = {}\n",
    "headers = {}\n",
    "\n",
    "response = requests.request(\"GET\", fx_url, headers=headers, data=payload,verify=False)\n",
    "response.status_code\n",
    "response.text\n",
    "x=json.loads(response.text)\n",
    "USD_NZD = round(x['rates']['USDNZD']['rate'],2)\n",
    "print('USD_NZD: '+str(USD_NZD))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_comparison(combined_list, exchange_rate):\n",
    "  price_data_new = pandas.DataFrame(combined_list, columns =[ 'result_name', 'release_yr','latest_captured_price_usd','latest_captured_timestamp', 'url' ])\n",
    "  price_data_new = price_data_new.drop_duplicates(subset='result_name')\n",
    "\n",
    "  #price_data_old = pandas.read_csv(currentdir+'\\\\'+ extract_file_name)\n",
    "  last_week_file = sorting_file(last_week_files)\n",
    "\n",
    "  try:\n",
    "    price_data_last_week = pandas.read_csv(currentdir+'\\\\file\\\\'+ last_week_file[-7])\n",
    "  except Exception: #Get the latest file found if there is no file 7days ago\n",
    "    latest_file_num = len(last_week_files) * -1\n",
    "    price_data_last_week = pandas.read_csv(currentdir+'\\\\file\\\\'+ last_week_file[latest_file_num])\n",
    "  \n",
    "  #Check if column is available before drop\n",
    "  if 'last_week_price_usd' in price_data_last_week.columns:\n",
    "    price_data_last_week = price_data_last_week.drop(['previous_price_usd','previous_captured_timestamp','last_week_price_usd','last_week_captured_timestamp'],axis=1)\n",
    "  else:\n",
    "    price_data_last_week = price_data_last_week.drop(['previous_price_usd','previous_captured_timestamp'],axis=1)\n",
    "\n",
    "  price_data_last_week_renamed = price_data_last_week.rename( \n",
    "  columns ={\n",
    "    \"latest_captured_price_usd\": \"last_week_price_usd\",\n",
    "    \"latest_captured_timestamp\": \"last_week_captured_timestamp\"\n",
    "})\n",
    "\n",
    "  price_data_old = pandas.read_csv(currentdir+'\\\\'+ 'price_list_latest.csv')\n",
    "  price_data_old = price_data_old.drop(['previous_price_usd','previous_captured_timestamp'],axis=1)\n",
    "  price_data_old_renamed = price_data_old.rename( \n",
    "  columns ={\n",
    "    \"latest_captured_price_usd\": \"previous_price_usd\",\n",
    "    \"latest_captured_timestamp\": \"previous_captured_timestamp\"\n",
    "})\n",
    "\n",
    "  result = pandas.merge(price_data_new[['result_name','release_yr', 'latest_captured_price_usd','latest_captured_timestamp',  'url']],\n",
    "                      price_data_old_renamed[['result_name', 'previous_price_usd','previous_captured_timestamp']],\n",
    "                      on = 'result_name',\n",
    "                      how ='left')\n",
    "  result['latest_captured_price_usd'] = pandas.to_numeric(result['latest_captured_price_usd']).round(2)\n",
    "  result['is_cheaper'] = np.where(result['latest_captured_price_usd']< result['previous_price_usd'],True,False)\n",
    "  result['difference'] = (result['latest_captured_price_usd']-result['previous_price_usd']).round(2)\n",
    "\n",
    "#ToDo change to weekly on Friday\n",
    "  result_last_week = pandas.merge(result[['result_name','release_yr', 'latest_captured_price_usd','latest_captured_timestamp','is_cheaper','difference']],\n",
    "                      price_data_last_week_renamed[['result_name', 'last_week_price_usd','last_week_captured_timestamp']],\n",
    "                      on = 'result_name',\n",
    "                      how ='left')\n",
    "  result_last_week['latest_captured_price_usd'] = pandas.to_numeric(result_last_week['latest_captured_price_usd']).round(2)\n",
    "  result_last_week['last_week_price_usd'] = pandas.to_numeric(result_last_week['last_week_price_usd']).round(2)\n",
    "  result_last_week['is_cheaper_last_week'] = np.where(result_last_week['latest_captured_price_usd']< result_last_week['last_week_price_usd'],True,False)\n",
    "  result_last_week['difference_last_week'] = (result_last_week['latest_captured_price_usd']-result_last_week['last_week_price_usd']).round(2)\n",
    "\n",
    "  result_cheaper = result[result['difference'] <-1]\n",
    "  result_cheaper_last_week = result_last_week[result_last_week['difference_last_week'] <-1]\n",
    "\n",
    " # result_cheaper[result_cheaper.select_dtypes(include=['number']).columns].round(2) *= exchange_rate\n",
    " # result_cheaper_last_week[result_cheaper_last_week.select_dtypes(include=['number']).columns] *= exchange_rate\n",
    "  result_cheaper[result_cheaper.select_dtypes(include=['number']).columns] = result_cheaper[result_cheaper.select_dtypes(include=['number']).columns].round(2) * exchange_rate\n",
    "  result_cheaper_last_week[result_cheaper_last_week.select_dtypes(include=['number']).columns] = result_cheaper_last_week[result_cheaper_last_week.select_dtypes(include=['number']).columns].round(2) * exchange_rate\n",
    "    \n",
    "  result.to_csv(currentdir+'\\\\file\\\\price_list_{result_datetime}.csv'.format(result_datetime = result_datetime),index=False)\n",
    "  result.to_csv(currentdir+'\\\\price_list_latest.csv',index=False)  \n",
    "  result_cheaper.to_csv(currentdir+'\\\\cheaper_price_list_latest.csv',index=False)\n",
    "\n",
    "  result_last_week.to_csv(currentdir+'\\\\price_list_last_week.csv',index=False)  \n",
    "  result_cheaper_last_week.to_csv(currentdir+'\\\\cheaper_price_list_last_week.csv',index=False)\n",
    "\n",
    "  return result_cheaper, result_cheaper_last_week\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_result_matched\n",
    "def get_result_matched(url_list, name_list):\n",
    "  result_list = []\n",
    "  price_usd_list = []\n",
    "  captured_timestamp_list = []\n",
    "  release_yr = []\n",
    "  url = []\n",
    "  for search_url in url_list:\n",
    "    response = requests.get(search_url)\n",
    "    if response.status_code ==200:\n",
    "      data = response.text\n",
    "      soup = BeautifulSoup(data,'html.parser')\n",
    "      link = soup.findAll(\"div\", {\"class\": \"product-list-price buy_cell\"})\n",
    "      link_yr = soup.findAll(\"div\", {\"class\": \"product-list-year\"})\n",
    "    else:\n",
    "      print(f\"Failed to get data via {url_list}\")\n",
    "    \n",
    "    for i in link_yr:\n",
    "      release_yr.append(i.string)\n",
    "    for i in link:\n",
    "      result_name = i.contents[1]['href'].replace(\"https://cdkeyprices.com/item/\",\"\").replace(\"-cd-key-xbox\",\"\").replace(\"-\",\" \")\n",
    "      t = i.contents[1]\n",
    "      #print(t)\n",
    "      if len(t) !=2:#in case there is a result return without price data\n",
    "        print('skip '+(result_name.title()) + ' as no data return')\n",
    "        continue\n",
    "      \n",
    "      result_list.append((result_name.title()))\n",
    "      t2 = t.contents[1]\n",
    "      #price_usd_list.append(i.contents[1].contents[1]['data-price-usd-value'])\n",
    "      price_usd_list.append(t2['data-price-usd-value'])\n",
    "      url.append(search_url)\n",
    "      captured_timestamp = datetime.datetime.now(tz=None)\n",
    "      captured_timestamp_list.append(captured_timestamp.strftime(\"%x %X\"))\n",
    "      \n",
    "  combined_list = list(zip(result_list, release_yr, price_usd_list, captured_timestamp_list, url))\n",
    "  #combined_list = list(zip(result_list, release_yr, price_usd_list, captured_timestamp_list))\n",
    "  \n",
    "  filtered_list = []\n",
    "  for name in name_list:\n",
    "    clean_name = re.sub(r'[^a-zA-Z0-9]+', ' ', name)\n",
    "\n",
    "    for res, yr, price, time, url in combined_list:\n",
    "        if clean_name.lower() in res.lower():\n",
    "            filtered_list.append((res, yr, price, time, url))    \n",
    "\n",
    "  print('Search result is generated')                \n",
    "  return filtered_list\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename_columns_to_lower_case\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def rename_columns_to_lower_case():\n",
    "    directory = currentdir+'\\\\file'\n",
    "    # get all the CSV files in the directory\n",
    "    csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]\n",
    "    \n",
    "    for file in csv_files:\n",
    "        # create a new filename for the modified CSV\n",
    "        new_filename = file.split('.')[0] + '_modified.csv'\n",
    "        \n",
    "        # open the input and output CSV files\n",
    "        with open(os.path.join(directory, file), 'r') as input_file, open(os.path.join(directory, new_filename), 'w', newline='') as output_file:\n",
    "            reader = csv.reader(input_file)\n",
    "            writer = csv.writer(output_file)\n",
    "            \n",
    "            # read the header row and convert all column names to lower case\n",
    "            header = next(reader)\n",
    "            header = [column.lower() for column in header]\n",
    "            \n",
    "            # write the modified header to the output file\n",
    "            writer.writerow(header)\n",
    "            \n",
    "            # copy the remaining rows from the input file to the output file\n",
    "            for row in reader:\n",
    "                writer.writerow(row)\n",
    "                \n",
    "        # remove the original CSV file and rename the modified CSV file to the original filename\n",
    "        os.remove(os.path.join(directory, file))\n",
    "        os.rename(os.path.join(directory, new_filename), os.path.join(directory, file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find_lowest_price from csvs\n",
    "def find_lowest_price(result_name, result_column_name='result_name', price_column_name='latest_captured_price_usd', date_column_name='latest_captured_timestamp'):\n",
    "    lowest_price_NZD = None\n",
    "    lowest_price_USD = None\n",
    "    lowest_date = None\n",
    "    search_name = None\n",
    "    csv_dir = currentdir+'\\\\file'\n",
    "    # get a list of all CSV files in the directory\n",
    "    files = os.listdir(csv_dir)\n",
    "    csv_files = [f for f in files if f.endswith('.csv')]\n",
    "\n",
    "    # loop over each CSV file\n",
    "    for file in csv_files:\n",
    "\n",
    "        # open the CSV file and create a CSV reader object\n",
    "        with open(os.path.join(csv_dir, file), 'r') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "\n",
    "            # loop over each row in the CSV file\n",
    "            for row in reader:             \n",
    "\n",
    "                # check if the result name matches the input\n",
    "                if result_name.lower() in row[result_column_name].lower():\n",
    "                    #print(row)\n",
    "                    # convert the price to an integer\n",
    "                    if row[price_column_name] is not None:\n",
    "                        price = float(row[price_column_name])\n",
    "\n",
    "                        # update the lowest price if it is None or greater than the current price\n",
    "                        if lowest_price_USD is None or price < lowest_price_USD:\n",
    "                            lowest_price_NZD = round(price * USD_NZD,2)\n",
    "                            lowest_price_USD = round(price,2)\n",
    "                            lowest_date = row[date_column_name]\n",
    "                            search_name = row[result_column_name]\n",
    "\n",
    "    return search_name,'NZD'+str(lowest_price_NZD),'USD'+str(lowest_price_USD), lowest_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('World Of Final Fantasy Maxima', 'NZD11.86', 'USD7.32', '04/03/23 15:18:53')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_lowest_price('World Of Final Fantasy Maxima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip as bought 'CODE VEIN'\n",
      " already\n",
      "skip as bought 'DRAGON BALL Z: KAKAROT'\n",
      " already\n",
      "skip as bought 'SCARLET NEXUS'\n",
      " already\n",
      "skip as bought 'Hades'\n",
      " already\n",
      "skip as bought 'Sekiro'\n",
      " already\n",
      "url list and name list are generated\n",
      "skip Dragon Quest Xi Echoes Of An Elusive Age as no data return\n",
      "skip Assassins Creed Valhalla The Siege Of Paris as no data return\n",
      "skip Assassins Creed Valhalla Wrath Of The Druids as no data return\n",
      "skip Assassins Creed Valhalla The Way Of The Berserker as no data return\n",
      "skip Azure Striker Gunvolt Striker Pack as no data return\n",
      "skip Azure Striker Gunvolt as no data return\n",
      "skip Bayonetta Origins Cereza And The Lost Demon as no data return\n",
      "skip Bayonetta 3 as no data return\n",
      "skip Bayonetta 2 as no data return\n",
      "skip Forza Horizon 4 Barrett Jackson Car Pack as no data return\n",
      "skip Forza Horizon 4 1959 Cadillac Eldorado Biarritz Convertible as no data return\n",
      "skip Forza Horizon 4 1963 Opel Kadett A as no data return\n",
      "skip Forza Horizon 4 1977 Hoonigan Ford Gymkhana 10 F 150 as no data return\n",
      "skip Forza Horizon 4 2019 Porsche 911 Carrera S as no data return\n",
      "skip Gunvolt Chronicles Luminous Avenger Ix as no data return\n",
      "skip Lost Judgment The Kaito Files as no data return\n",
      "skip Judgment Apocalypse Survival Simulation as no data return\n",
      "skip Resident Evil Revelations 2 Episode 3 Judgment as no data return\n",
      "skip Star Trek Judgment Rites as no data return\n",
      "skip Ni No Kuni Ii Revenant Kingdom as no data return\n",
      "skip Ni No Kuni Ii Revenant Kingdom The Princes Edition as no data return\n",
      "skip Nights Of Azure as no data return\n",
      "skip Ni No Kuni Wrath Of The White Witch as no data return\n",
      "skip Nier Automata The End Of Yorha Edition as no data return\n",
      "skip Necrobarista Final Pour as no data return\n",
      "skip Nier Automata Game Of The Yorha Edition as no data return\n",
      "skip Nier Automata as no data return\n",
      "skip Nier Automata 3C3C1D119440927 as no data return\n",
      "skip Tavernier as no data return\n",
      "skip Dracula 2 The Last Sanctuary as no data return\n",
      "skip No More Heroes Iii as no data return\n",
      "skip Octopath Traveler Ii as no data return\n",
      "skip Overcooked 2 Campfire Cook Off as no data return\n",
      "skip Overcooked 2 Night Of The Hangry Horde as no data return\n",
      "skip Overcooked 2 Carnival Of Chaos as no data return\n",
      "skip Overcooked 2 Too Many Cooks as no data return\n",
      "skip Overcooked 2 Surf N Turf as no data return\n",
      "skip Overcooked 2 Too Many Cooks Pack as no data return\n",
      "skip Persona 5 Royal Launch Edition as no data return\n",
      "skip Sd Gundam Battle Alliance Deluxe Edition as no data return\n",
      "skip Soul Hackers 2 Digital Premium Edition as no data return\n",
      "skip Soul Hackers 2 Digital Deluxe Edition as no data return\n",
      "skip Tales Of Arise Collaboration Costume Pack as no data return\n",
      "skip Yakuza 6 Song Of Life Clan Creator Card Bundle as no data return\n",
      "skip The Yakuza Remastered Collection Day One Edition as no data return\n",
      "skip A Hat In Time Nyakuza Metro as no data return\n",
      "skip Yakuza Kiwami 2 Clan Creator Bundle as no data return\n",
      "skip Yakuza Kiwami Steelbook Edition as no data return\n",
      "skip Yakuza Dead Souls as no data return\n",
      "skip Yakuza as no data return\n",
      "skip Yakuza Empire as no data return\n",
      "skip Yakuza Undisputed as no data return\n",
      "skip Yakuza Like A Dragon Job Set as no data return\n",
      "skip Rune Factory 4 as no data return\n",
      "skip Etrian Odyssey Origins Collection as no data return\n",
      "skip 7Days Origins as no data return\n",
      "skip Rorys Restaurant Origins as no data return\n",
      "skip One Piece Odyssey Traveling Outfit Set as no data return\n",
      "skip Sifu Digital Deluxe Edition as no data return\n",
      "Search result is generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\303435\\AppData\\Local\\Temp\\ipykernel_22212\\2354911962.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result_cheaper[result_cheaper.select_dtypes(include=['number']).columns] = result_cheaper[result_cheaper.select_dtypes(include=['number']).columns].round(2) * exchange_rate\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                   result_name release_yr  \\\n",
       " 4      Assassins Creed Valhalla Deluxe Edition       2022   \n",
       " 12  Bayonetta Vanquish 10Th Anniversary Bundle       2022   \n",
       " 28              Forza Horizon 5 Deluxe Edition       2018   \n",
       " 38                              Nier Replicant       2021   \n",
       " 41               Ori And The Will Of The Wisps       2020   \n",
       " 65             Devil May Cry 5 Special Edition       2016   \n",
       " 66                       Red Dead Redemption 2       2010   \n",
       " 78               World Of Final Fantasy Maxima       2018   \n",
       " \n",
       "     latest_captured_price_usd latest_captured_timestamp  \\\n",
       " 4                     46.9314         04/25/23 09:33:40   \n",
       " 12                    27.3618         04/25/23 09:33:44   \n",
       " 28                    69.2388         04/25/23 09:33:46   \n",
       " 38                   102.0924         04/25/23 09:33:52   \n",
       " 41                    11.2428         04/25/23 09:33:56   \n",
       " 65                    20.1204         04/25/23 09:34:04   \n",
       " 66                    22.2750         04/25/23 09:34:04   \n",
       " 78                    24.1380         04/25/23 09:34:09   \n",
       " \n",
       "                                                   url  previous_price_usd  \\\n",
       " 4   https://cdkeyprices.com/search?sk=Assassins+Cr...             52.0506   \n",
       " 12  https://cdkeyprices.com/search?sk=Bayonetta&ca...             64.4112   \n",
       " 28  https://cdkeyprices.com/search?sk=Forza+Horizo...             83.7378   \n",
       " 38  https://cdkeyprices.com/search?sk=NieR+Replica...            104.1498   \n",
       " 41  https://cdkeyprices.com/search?sk=Ori+and+the+...             14.7258   \n",
       " 65  https://cdkeyprices.com/search?sk=Devil+May+Cr...             27.0540   \n",
       " 66  https://cdkeyprices.com/search?sk=Red+Dead+Red...             37.0980   \n",
       " 78  https://cdkeyprices.com/search?sk=world+of+fin...             27.4752   \n",
       " \n",
       "    previous_captured_timestamp  is_cheaper  difference  \n",
       " 4            04/24/23 15:15:45        True     -5.1192  \n",
       " 12           04/24/23 15:15:49        True    -37.0494  \n",
       " 28           04/24/23 15:15:53        True    -14.4990  \n",
       " 38           04/24/23 15:15:59        True     -2.0574  \n",
       " 41           04/24/23 15:16:03        True     -3.4830  \n",
       " 65           04/24/23 15:16:12        True     -6.9336  \n",
       " 66           04/24/23 15:16:13        True    -14.8230  \n",
       " 78           04/24/23 15:16:18        True     -3.3372  ,\n",
       " Empty DataFrame\n",
       " Columns: [result_name, release_yr, latest_captured_price_usd, latest_captured_timestamp, is_cheaper, difference, last_week_price_usd, last_week_captured_timestamp, is_cheaper_last_week, difference_last_week]\n",
       " Index: [])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run\n",
    "#url_list = get_url(search_list)\n",
    "#combined_list = get_result(url_list)\n",
    "#combined_list = get_result_top_5(url_list)\n",
    "#url_list = get_url(search_list)\n",
    "url_list, name_list =get_url_v2(search_list)\n",
    "filtered_list = get_result_matched(url_list,name_list)\n",
    "#exchange_rate = get_currency('USD', 'NZD') #There is a bug on the api, so read it directly instead of via py pkg\n",
    "gen_comparison(filtered_list,USD_NZD)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PriceCheckerV2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "428a7b1871b7a5804c0be3f6f645112f4843b5f75c6557809d186f7b22dd100b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
